Examples for S-AMA Code
================

``` r
library(cvxclustr)
```

    ## Loading required package: Matrix

    ## Loading required package: igraph

    ## 
    ## Attaching package: 'igraph'

    ## The following objects are masked from 'package:stats':
    ## 
    ##     decompose, spectrum

    ## The following object is masked from 'package:base':
    ## 
    ##     union

``` r
library(scvxclustr)
```

    ## Loading required package: mclust

    ## Package 'mclust' version 5.4.2
    ## Type 'citation("mclust")' for citing this R package in publications.

    ## Loading required package: gglasso

    ## 
    ## Attaching package: 'scvxclustr'

    ## The following objects are masked from 'package:cvxclustr':
    ## 
    ##     AMA_step_size, create_adjacency, create_clustering_problem,
    ##     find_clusters

``` r
###Generate a simple data matrix used for clustering, of which the rows are samples and the 
###columns are variables. Here we produce a matrix consisted of 80 observations and 100 
###variables(the first 20 are informative variables). There exist 2 classes in this data.

set.seed(1)
class1 <- sample(1:80, 40)
r1 <- rep(0, 80)
r1[class1] <- 1

sample1_matrix <- matrix(0, nrow = 80, ncol = 20)
for (i in which(r1 == 0)) {
  sample1_matrix[i, ] <- rnorm(20, mean = 1, sd = 0.5) 
}

for (i in which(r1 == 1)) {
  sample1_matrix[i, ] <- rnorm(20, mean = -1, sd = 0.5)  
}

sample2_matrix <- matrix(rnorm(800, mean = 4, sd = 1), nrow = 80, ncol = 10)
sample_matrix <- cbind(sample1_matrix, sample2_matrix)
sample_scale <- scale(sample_matrix, scale = F)

n <- nrow(sample_matrix)
p <- ncol(sample_matrix)

###Construct sparse weights at first and then use it to take convex clustering by AMA 
###algorithm from package "cvxclustr". In this step we get the sparse weights for gamma_1 
###and the weights for gamma_2

weight <- kernel_weights(t(sample_scale), 0.5)
weight <- knn_weights(weight, 5, n)
weight <- weight/(sqrt(p) * sum(weight))

Gamma1 <- 1e15
Gamma2 <- 10

cvxresult <- cvxclust(t(sample_scale), nu = 0.01, weight, gamma = Gamma1, method = "ama", accelerate = F)$U[[1]]
cvxresult <- t(cvxresult)

mu <- vector()
for (j in 1:p) {
  mu[j] <- 1/(sqrt(sum(cvxresult[, j]^2)) + 1e-6)
}

Gamma2_weight <- mu/(sqrt(n)*sum(mu))

nu <- AMA_step_size(weight, n)/2


###In this step, we get the sparse convex clustering result and put it into the same function 
###"create_adjacency" to get the adjacency matrix so that we can get final clustering classes.
###


system.time(scvx_result <- scvxclust(X = sample_scale, weight, nu = nu, Gamma1 = Gamma1, Gamma2 = Gamma2, Gamma2_weight = Gamma2_weight, method = "ama", max_iter = 1e4, tol_abs = 1e-5)) 
```

    ##    user  system elapsed 
    ##   0.022   0.000   0.023

``` r
A <- create_adjacency(scvx_result$V[[1]], weight, 80, method = "ama")
cluster <- find_clusters(A)
cluster
```

    ## $cluster
    ##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
    ## [24] 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46
    ## [47] 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69
    ## [70] 70 71 72 73 74 75 76 77 78 79 80
    ## 
    ## $size
    ##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
    ## [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
    ## [71] 1 1 1 1 1 1 1 1 1 1

``` r
scvx_result$V[[1]][, 1:10]
```

    ##                [,1]          [,2]          [,3]          [,4]
    ##  [1,] -2.323355e-07 -5.111716e-07 -7.949276e-07  8.006737e-07
    ##  [2,] -1.873321e-07 -4.194759e-07 -6.474810e-07  6.576045e-07
    ##  [3,] -5.829351e-08 -1.375211e-07 -2.171792e-07  2.107061e-07
    ##  [4,]  3.321437e-09  6.627721e-09  2.510486e-08  1.298808e-09
    ##  [5,] -2.075521e-08 -4.567574e-08 -6.292176e-08  7.891054e-08
    ##  [6,] -9.828582e-08 -2.096374e-07 -3.141394e-07  3.387524e-07
    ##  [7,]  4.390831e-08  1.031093e-07  1.580955e-07 -1.613135e-07
    ##  [8,] -7.944240e-08 -1.740478e-07 -2.610068e-07  2.818842e-07
    ##  [9,] -3.451423e-08 -7.217708e-08 -1.023142e-07  1.217525e-07
    ## [10,] -2.163101e-08 -4.748975e-08 -7.464339e-08  7.381831e-08
    ## [11,]  3.085883e-08  7.149236e-08  1.087912e-07 -1.124277e-07
    ## [12,] -1.991570e-07 -4.541365e-07 -7.043581e-07  7.054188e-07
    ## [13,] -4.780026e-08 -9.847589e-08 -1.493779e-07  1.601707e-07
    ## [14,]  4.354614e-07  9.691751e-07  1.512035e-06 -1.506659e-06
    ## [15,]  2.913228e-07  6.471584e-07  9.917356e-07 -1.019787e-06
    ## [16,] -1.644296e-08 -2.870947e-08 -5.211026e-08  4.339685e-08
    ## [17,]  3.423877e-08  7.407914e-08  1.167191e-07 -1.128962e-07
    ## [18,]  2.541023e-07  5.692366e-07  8.857910e-07 -8.858314e-07
    ## [19,]  5.204928e-08  1.050120e-07  1.586056e-07 -1.708973e-07
    ## [20,]  3.168731e-07  7.032563e-07  1.087347e-06 -1.101441e-06
    ## [21,]  1.522024e-05  2.868868e-05  4.404733e-05 -4.720254e-05
    ## [22,]  2.523420e-07  5.243663e-07  8.015405e-07 -8.347589e-07
    ## [23,] -2.566124e-07 -5.841268e-07 -8.428579e-07  9.551766e-07
    ## [24,]  2.511556e-07  5.954158e-07  9.313918e-07 -9.080347e-07
    ## [25,] -3.591404e-07 -5.804282e-07 -5.349176e-07  5.334562e-07
    ## [26,] -5.925490e-06 -1.334109e-05 -2.070590e-05  2.074084e-05
    ## [27,]  1.388031e-06  3.095909e-06  4.799441e-06 -4.839162e-06
    ## [28,] -3.284549e-07 -7.372955e-07 -1.129254e-06  1.158455e-06
    ## [29,]  1.009194e-07  2.406252e-07  3.574930e-07 -3.746123e-07
    ## [30,] -5.316621e-07 -1.260258e-06 -1.927189e-06  1.926751e-06
    ##                [,5]          [,6]          [,7]          [,8]
    ##  [1,] -2.768941e-07  7.877182e-08 -1.195547e-07  4.577710e-07
    ##  [2,] -2.270274e-07 -7.165653e-08  1.164198e-07 -3.754031e-07
    ##  [3,] -6.893330e-08  1.203462e-08 -2.258011e-08  4.070303e-08
    ##  [4,] -6.157202e-09 -4.418865e-08  6.763707e-08 -2.755281e-07
    ##  [5,] -3.064160e-08 -6.667770e-08  1.126608e-07 -3.512499e-07
    ##  [6,] -1.236564e-07  6.602985e-08 -1.064451e-07  3.638084e-07
    ##  [7,]  5.475923e-08 -8.623031e-08  1.339177e-07 -4.765811e-07
    ##  [8,] -1.017371e-07 -1.240497e-07  1.977971e-07 -7.119592e-07
    ##  [9,] -4.711378e-08  4.407326e-09 -9.807543e-09  3.527979e-09
    ## [10,] -2.524798e-08 -1.606062e-07  2.504056e-07 -9.866378e-07
    ## [11,]  3.871168e-08  7.879755e-08 -1.313174e-07  4.551510e-07
    ## [12,] -2.396155e-07  9.191498e-08 -1.435549e-07  5.370454e-07
    ## [13,] -5.898509e-08 -5.197915e-08  7.416644e-08 -3.286530e-07
    ## [14,]  5.152255e-07  6.268967e-09 -7.431423e-09  4.468535e-08
    ## [15,]  3.561390e-07 -1.362476e-07  2.179624e-07 -8.199901e-07
    ## [16,] -1.454271e-08 -2.022146e-08  3.250435e-08 -1.056505e-07
    ## [17,]  3.847367e-08  1.631061e-07 -2.647189e-07  9.277919e-07
    ## [18,]  3.027886e-07 -5.210134e-08  8.082658e-08 -3.224304e-07
    ## [19,]  6.373751e-08 -1.182072e-07  1.841491e-07 -6.800678e-07
    ## [20,]  3.811362e-07  7.852908e-08 -1.234552e-07  4.797051e-07
    ## [21,]  1.801068e-05 -5.305154e-06  1.103321e-05 -2.539744e-05
    ## [22,]  3.022696e-07  3.964589e-07 -6.239919e-07  2.262709e-06
    ## [23,] -3.497746e-07  2.068798e-08 -1.083675e-08  5.667936e-08
    ## [24,]  2.981396e-07 -6.793179e-07  1.093714e-06 -3.779375e-06
    ## [25,] -1.886108e-07  1.616025e-07 -3.527762e-07  5.013807e-07
    ## [26,] -7.085420e-06  2.567179e-07 -3.620626e-07  1.129408e-06
    ## [27,]  1.665708e-06 -8.966856e-07  1.394999e-06 -5.225539e-06
    ## [28,] -4.023757e-07  6.603592e-07 -1.059535e-06  3.784135e-06
    ## [29,]  1.292753e-07 -3.156729e-07  5.004847e-07 -2.024989e-06
    ## [30,] -6.458862e-07 -2.189629e-08 -1.129944e-07 -3.032951e-07
    ##                [,9]         [,10]
    ##  [1,]  7.808353e-08  3.815267e-07
    ##  [2,] -8.792659e-08 -2.735295e-07
    ##  [3,]  1.412831e-08  1.314779e-08
    ##  [4,] -4.988679e-08 -2.364536e-07
    ##  [5,] -7.640604e-08 -2.592512e-07
    ##  [6,]  8.639982e-08  2.743134e-07
    ##  [7,] -8.539890e-08 -3.813096e-07
    ##  [8,] -1.471452e-07 -5.674760e-07
    ##  [9,] -5.087043e-09 -5.055969e-09
    ## [10,] -2.072443e-07 -8.184697e-07
    ## [11,]  1.019173e-07  3.564959e-07
    ## [12,]  9.985164e-08  4.416419e-07
    ## [13,] -5.316296e-08 -2.906823e-07
    ## [14,]  4.060668e-09  4.452299e-08
    ## [15,] -1.854071e-07 -6.610857e-07
    ## [16,] -2.572999e-08 -7.629942e-08
    ## [17,]  2.026446e-07  7.260044e-07
    ## [18,] -6.578014e-08 -2.700573e-07
    ## [19,] -1.451812e-07 -5.414396e-07
    ## [20,]  1.070148e-07  3.919500e-07
    ## [21,] -7.074485e-06 -1.546496e-05
    ## [22,]  4.110531e-07  1.839529e-06
    ## [23,] -6.632756e-08  7.769523e-08
    ## [24,] -8.084659e-07 -2.930280e-06
    ## [25,]  5.577616e-07 -2.121170e-07
    ## [26,]  1.913352e-07  7.787728e-07
    ## [27,] -1.097649e-06 -4.206373e-06
    ## [28,]  7.743116e-07  3.015922e-06
    ## [29,] -3.770721e-07 -1.749026e-06
    ## [30,]  2.288314e-07 -5.910970e-07

``` r
###construct sparse weights at first and then use it to take convex clustering by AMA 
###algorithm from package "cvxclustr"


weight <- kernel_weights(t(sample_matrix), 0.5)
weight <- knn_weights(weight, 5, n)
weight <- weight/(sqrt(p) * sum(weight))


cvx_result <- cvxclust(X = t(sample_matrix), w = weight, gamma = 1e14, method = "ama", nu = 0.01)


###construct adjacency matrix to get final clustering results. Here we can see the clustering 
###results by object "cluster", and we find that most observations are classified into 2 classes.

A <- create_adjacency(cvx_result$V[[1]], weight, n, method = "ama")
cluster <- find_clusters(A)
cluster
```

    ## $cluster
    ##  [1] 1 2 2 2 1 2 2 1 1 2 2 2 1 2 1 1 1 1 2 2 1 1 2 1 1 1 2 2 2 1 1 2 1 2 1
    ## [36] 2 2 1 1 2 2 2 2 2 1 1 1 1 1 2 1 2 2 2 2 2 1 2 2 2 2 1 1 1 2 2 2 1 1 1
    ## [71] 1 1 2 1 2 2 1 1 1 1
    ## 
    ## $size
    ## [1] 40 40

``` r
cvx_result$V[[1]][, 1:10]
```

    ##       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
    ##  [1,]    0    0    0    0    0    0    0    0    0     0
    ##  [2,]    0    0    0    0    0    0    0    0    0     0
    ##  [3,]    0    0    0    0    0    0    0    0    0     0
    ##  [4,]    0    0    0    0    0    0    0    0    0     0
    ##  [5,]    0    0    0    0    0    0    0    0    0     0
    ##  [6,]    0    0    0    0    0    0    0    0    0     0
    ##  [7,]    0    0    0    0    0    0    0    0    0     0
    ##  [8,]    0    0    0    0    0    0    0    0    0     0
    ##  [9,]    0    0    0    0    0    0    0    0    0     0
    ## [10,]    0    0    0    0    0    0    0    0    0     0
    ## [11,]    0    0    0    0    0    0    0    0    0     0
    ## [12,]    0    0    0    0    0    0    0    0    0     0
    ## [13,]    0    0    0    0    0    0    0    0    0     0
    ## [14,]    0    0    0    0    0    0    0    0    0     0
    ## [15,]    0    0    0    0    0    0    0    0    0     0
    ## [16,]    0    0    0    0    0    0    0    0    0     0
    ## [17,]    0    0    0    0    0    0    0    0    0     0
    ## [18,]    0    0    0    0    0    0    0    0    0     0
    ## [19,]    0    0    0    0    0    0    0    0    0     0
    ## [20,]    0    0    0    0    0    0    0    0    0     0
    ## [21,]    0    0    0    0    0    0    0    0    0     0
    ## [22,]    0    0    0    0    0    0    0    0    0     0
    ## [23,]    0    0    0    0    0    0    0    0    0     0
    ## [24,]    0    0    0    0    0    0    0    0    0     0
    ## [25,]    0    0    0    0    0    0    0    0    0     0
    ## [26,]    0    0    0    0    0    0    0    0    0     0
    ## [27,]    0    0    0    0    0    0    0    0    0     0
    ## [28,]    0    0    0    0    0    0    0    0    0     0
    ## [29,]    0    0    0    0    0    0    0    0    0     0
    ## [30,]    0    0    0    0    0    0    0    0    0     0
